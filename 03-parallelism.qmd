# Parallel computing

...

Above all, my question is: how can I reduce the time to train machine learning models by utilizing the handful of CPU cores (and maybe even the snazzy GPU) on my laptop?

```{r}
#| label: setup-03
#| include: false
if (!identical(Sys.getenv("emlwr.eval_fits"), "true")) {
  eval_fits <- FALSE
} else {
  eval_fits <- TRUE
}

library(tidymodels)
library(future)
library(bench)

knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE
)

if (!eval_fits) {
  load("data/03-parallelism/t_laptop.Rda")
  load("data/03-parallelism/t_cloud.Rda")
}
```

## Across-models {#sec-across-models}

In @sec-intro, one of the changes I made to greatly speed up that resampling process was to introduce an across-models parallel backend. By "across-models," I mean that each individual model fit happens on a single CPU core, but I allot each of the CPU cores I've reserved for training a number of model fits to take care of.

Phrased another way, in the case of "sequential" training, all `r 10 * 12` model fits happened one after the other.

![](figures/basic_resample.png){#fig-basic-resample-2}

While the one CPU core running my R process works itself to the bone, the other remaining 9 are mostly sitting idle (besides keeping my many browser tabs whirring). CPU parallelism is about somehow making use of more cores than the one my main R process is running on; in theory, if $n$ times as many cores are working on fitting models, the whole process could take $1/n$ of the time.

![](figures/parallel_resample.png){#fig-parallel-resample-2}

Why do I say "in theory"? The orchestration of splitting up that work is actually a very, very difficult problem, for two main reasons:

-   **Getting data from one place to another**: Each of us has likely spent minutes or even hours waiting for a call to `load()` on a big old `.RData` object to complete. In that situation, data is being brought in from our hard disk or a remote database (or even elsewhere in memory) into memory allocated by our R process. R is a single-process program, so in order to train multiple models simultaneously, we need multiple R processes, each able to access the training data. We then have two options: one would be to somehow allow each of those R processes to share one copy of the data (this is the idea behind forking, described in @sec-forking), the other to send a copy of the data to each process (the idea behind socket clusters, described in @sec-socket-clusters). The former *sounds* nice but can become a headache quite quickly. The latter *sounds* computationally expensive but, with enough memory and sufficiently low latency in copying data (as would be the case with a set of clusters living on one laptop), can often outperform forking for local workflows.

-   **Load balancing**: Imagine I have some machine learning model with a hyperparameter $p$, and that the computational complexity of that model is such that, with hyperparameter value $p$, the model takes $p$ minutes to train. I am trying out values of $p$ in $1, 2, 3, ..., 40$ and distributing model training across 5 cores. Without the knowledge of how $p$ affects training times, I might send models with $p$ in $1, 2, 3, ...8$ off to the first core, $9, 10, 11, ..., 16$ off to the second core, and so on. In this case, the first core would finish up all of its fits in a little over half an hour while the last would take almost 5 hours. In this example, I've taken the penalty on overhead of sending all of the training data off to each core, but in the end, one core ends up doing the majority of the work anyway. In this case, too, we were lucky that the computational complexity of model fits relative to this parameter were roughly linearâ€”it's not uncommon for model fit times to have a quadratic or geometric relationship with the values of important hyperparameters. A critical reader might have two questions. The first: if the computational complexity relative to this parameter is known, why don't you just batch the values of $p$ up such that each worker will take approximately the same amount of time? This is a reasonable question, and it relates to the hard problem of *chunking*. In some situations, related to individual parameters, it really is just about this simple to determine the relationship between parameter values and fit times. In reality, those relationships tend not to be quite so clear-cut, and even when they are, the implications of that parameter value for fit times often depend on the values of other parameters; a pairing of some parameter value $p$ with some other value of a different parameter $q$ might cause instability in some gradient process or otherwise, making the problem of estimating the fit time of a model given some set of parameter values a pretty difficult problem for some model types. The second question: couldn't you just send each of the cores a single parameter value and have them let the parent R process know they're done, at which point they'll receive another parameter value to get to work on evaluating? That way, the workers that happen to end up with a quicker-fitting values earlier on won't sit idle waiting for other cores to finish. This approach is called *asynchronous* (or "async") and, in some situations, can be quite helpful. Remember, though, that this requires getting data (in the form of the communication that a given worker is done evaluating a model, and maybe passing along some performance metric values) back and forth much more often. If the overhead of that communication exceeds the time that synchronous workers had spent idle, waiting for busier cores to finish running, then the asynchronous approach will result in a net slowdown.

There are two dominant approaches to distributing model fits across local cores that I've hinted at already: forking and socket clusters. We'll delve further into the weeds of each of those approaches in the coming subsections. At a high level, though, the folk knowledge is that forking is subject to less overhead in sending data back and forth, but has some quirks that make it less portable (more plainly, it's not available on Windows) and a bit unstable thanks to a less-than-friendly relationship with R's garbage collector. As for load balancing, the choice between these two parallelism techniques isn't really relevant.

Before I spend time experimenting with these techniques, I want to quickly situate the terminology I'm using here in the greater context of discussions of parallel computing with R. "Sequential," "forking," and "socket clusters" are my preferred terms for the techniques I'll now write about, but there's quite a bit of diversity in the terminology folks use to refer to them. I've also called out keywords (as in, functions or packages) related to these techniques in various generations of parallel computing frameworks in R. In "base," I refer to functions in the parallel package, building on popular packages multicore (first on CRAN in 2009, inspiring `mclapply()`) and snow (first on CRAN in 2003, inspiring `parLapply()`) and included in base installations of R from 2011 onward [@pkgparallel].

| Technique | future | foreach | base | Synonyms |
|---------------|---------------|---------------|---------------|---------------|
| Sequential | `sequential()` |  |  | Serial |
| Forking | `multicore()` | doMC | `mclapply()` |  |
| Socket Clusters | `multisession()` | doParallel | `parLapply()` | Parallel Socket Clusters (PSOCK), Cluster, Socket |

### Sequential {#sec-sequential}

<!--# example -->

### Forking {#sec-forking}

Process forking is a mechanism where an R process creates an exact copy of itself, called a "child" process (or "worker.") Initially, workers share memory with the original ("parent") process, meaning that there's no overhead resulting from creating multiple copies of training data to send out to workers.

<!--# example -->

There are a few notable drawbacks of process forking:

-   There's no direct way to "fork" a process from one machine to another, so forking is available only on a single machine. To distribute computations across multiple machines, practitioners will need to make use of socket clusters.

-   Forking is based on the operating system command `fork`, available only on Unix-alikes (i.e. macOS and Linux). Windows users are out of luck.

-   In practice, memory that is initially shared often ends up ultimately copied due to R's garbage collection. "\[I\]f the garbage collector starts running in one of the forked \[workers\], or the \[parent\] process, then that originally shared memory can no longer be shared and the operating system starts copying memory blocks into each \[worker\]. Since the garbage collector runs whenever it wants to, there is no simple way to avoid this" [@pkgfuture].

### Socket Clusters {#sec-socket-clusters}

## Within-models {#sec-within-models}

I've only written so far about across-model parallelism, where multiple CPU cores are used to train a set of models but individual model fits happen on a single core. For most machine learning models available via tidymodels, this is the only type of parallelism possible. However, some of the most well-used modeling engines in tidymodels, like XGBoost and LightGBM, allow for distributing the computations to train a *single* model across several CPU cores (or even on GPUs). This begs the question, then, of whether tidymodels users should always stick to across-model parallelism, to within-model parallelism for the models it's available for, or to a hybrid of both. We'll focus first on the former two options in this subsection and then explore their interactions in @sec-within-and-across.

### CPU {#sec-openmp}

### GPU {#sec-gpu-training}

XGBoost parallelizes at a finer level (individual trees and split finding), while LightGBM parallelizes at a coarser level (features and data subsets).

XGBoost

-   arg `device` is `cpu` or `cuda` (or `gpu`, but `cuda` is the only supported `device`).

-   arg `nthread` is integer

-   uses openMP for cpu (?)

-   `gpu_hist` is apparently pretty ripping?

LightGBM

-   `device_type` is `cpu`, `gpu`, or `cuda` (fastest, but requires GPUs supporting CUDA)

-   `cuda` is fastest but only available on Linux with NVIDIA GPUs with compute capability 6.0+

-   `gpu` is based on OpenCL... M1 Pro is a "built-in" / "integrated"

-   `cpu` uses OpenMP for CPU parallelism

-   set to real CPU cores (i.e. not threads)

-   refer to [Installation Guide](https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html) to build LightGBM with GPU or CUDA support

-   are `num_threads` is integer

-   Dask available to Python users

aorsf

-   **`n_thread`** implements OpenMP CPU threading

keras (MLP)

-   CPU or GPU

-   cuda (available only for jax backend?)

-   also available with tensorflow

h2o stuff

baguette

-   `control_bag(allow_parallel)`? does this respect nested parallelism?

While GPU support is available for both libraries in R, it's not as straightforward to use as in Python. The R ecosystem generally has less robust GPU support compared to Python, which can make GPU-accelerated machine learning in R more challenging to set up and use.

## Within and Across {#sec-within-and-across}

## Distributed Computing

So far in this chapter, I've mostly focused on the performance considerations for distributing computations across cores on a single computer. Distributed computing "in the cloud," where data is shipped off for processing on several different computers, is a different ball-game. That conversation is mostly outside of the scope of this book, but I do want to give some high-level intuition on how local parallelism differs from distributed parallelism.

The idea that will get you the most mileage in reasoning about distributed parallelism is this: the overhead of sending data back and forth is much more substantial in distributed computing than it is in the local context.

We can demonstrate this with a short experiment. Let's define a function that tracks the elapsed time resample a boosted tree ensemble against simulated data, given a number of rows to simulate and a parallelism approach.

```{r}
#| label: time_resample_bt
time_resample_bt <- function(n_rows, plan) {
  # simulate data with n_rows rows
  set.seed(1)
  d <- sim_regression(n_rows)
  
  # set up a parallelism plan
  # set `workers = 4`, which will be ignored for `plan = "sequential"`.
  suppressWarnings(
    plan(plan, workers = 4) 
  )
  
  # track the elapsed time to...
  bench::mark(
    resample =
      fit_resamples(
        # ...evaluate a boosted tree ensemble...
        boost_tree("regression"),
        # ...modeling the outcome using all predictors...
        outcome ~ .,
        # ...against a 10-fold cross-validation of `d`.
        vfold_cv(d, v = 10)
      )
  )
}
```

Here's a quick example, simulating 100 rows of data and evaluating its resamples sequentially:

```{r}
#| label: t_seq
#| eval: !expr eval_fits
t_seq <- time_resample_bt(100, "sequential")

t_seq
```

```{r}
#| label: t_seq_print
#| eval: !expr (!eval_fits)
t_seq
```

In total, the whole process took `r t_seq$median[1]` on my laptop. This expression, among other things, fits a model for each resample on $n * \frac{v-1}{v} = 100 * \frac{9}{10} = 90$ rows, meaning that even if the model fits took up 100% of the evaluation time in total, they take `r t_seq$median[1] / 10` each. In other words, these fits are quite fast.

In order for me to benefit from distributing these computations across cores, the overhead of sending data out to workers has to be so minimal that it doesn't overtake the time saved in distributing model fits across cores. Let's see how this plays out on my laptop, first:

```{r}
#| label: t_par
#| eval: !expr eval_fits
t_par <- time_resample_bt(100, "multisession")

t_par
```

```{r}
#| label: t_par_print
#| eval: !expr (!eval_fits)
t_par
```

No dice! As we know from earlier on, though, we'll start to see a payoff when model fits take long enough to outweigh the overhead of sending data back and forth between workers. But, but! Remember the high-mileage lesson: this overhead is greater for distributed systems. Let's demonstrate this.

I'll first run this experiment for numbers of rows $100, 1000, ..., 1,000,000$ both sequentially and via socket clusters on my laptop, recording the timings as I do so. Then, I'll do the same thing on a popular data science hosted service, and we'll compare results.

```{r}
#| label: bench-press-time_resample_bt
#| eval: false
bench::press(
  time_resample_bt(n_rows, plan),
  n_rows = 10^(2:6),
  plan = c("sequential", "multisession")
)
```

```{r}
#| label: t_laptop
#| eval: !expr eval_fits
#| include: false
if (!file.exists("data/03-parallelism/t_cloud.Rda")) {
  cli::cli_abort(
    c(
      "Can't evaluate fits for this chapter without data from the
       cloud system.",
      "See `data/03-parallelism/time_resample_bt.Rda`."
    )
  )
}

t_laptop <-
  bench::press(
    time_resample_bt(n_rows, plan),
    n_rows = 10^(2:6),
    plan = c("sequential", "multisession")
  )
```

```{r}
#| label: save-t_laptop
#| eval: !expr eval_fits
#| include: false
t_seq$memory <- NULL
t_seq$result <- NULL

t_par$memory <- NULL
t_par$result <- NULL

t_laptop$memory <- NULL
t_laptop$result <- NULL

save(t_seq, t_par, t_laptop, file = "data/03-parallelism/t_laptop.Rda")
```

The resulting timings are in the object `timings` and look like this:

```{r}
#| label: make-timings
#| include: false
timings <- 
  bind_rows(
    t_laptop %>% mutate(system = "laptop"),
    t_cloud %>% mutate(system = "cloud")
  )
```

```{r}
#| label: print-timings
timings %>%
  select(n_rows, plan, median, system) %>%
  head()
```

There's one timing per unique combination of number of rows, parallelism plan, and system (`"laptop"` vs `"cloud"`). Based on these timings, we can calculate the factor of speedup for sequential evaluation versus its parallel analogue.

```{r}
#| label: plot-timings
timings %>%
  select(n_rows, plan, median, system) %>%
  pivot_wider(names_from = plan, values_from = median) %>%
  mutate(
    speedup = as.numeric(sequential / multisession),
    speedup = if_else(speedup < 1, -1/speedup, speedup)
  ) %>%
  ggplot() +
  aes(x = n_rows, y = speedup, col = system) +
  geom_point() +
  scale_x_log10() +
  labs(
    x = "Log(Number of Rows)", 
    y = "Signed Factor of Speedup", 
    col = "System"
  )
```

In this plot, a signed speedup value of 2 would mean that the socket cluster (i.e., parallel) approach was twice as fast, while a value of -2 would mean that the sequential approach ran twice as fast as the socket cluster approach. In general, larger numbers of rows (and thus longer-running model fits) tend to be associated with greater speedups as a result of switching to parallel computing. For local clusters on my laptop, the overhead of passing data around is small enough that I start to see a payoff when switching to parallel computing for only 1000 rows. As for the cloud system, though, model fits have to take a *long* time before switching to parallel computing begins to reduce elapsed times.

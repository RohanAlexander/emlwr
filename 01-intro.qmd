# Introduction {#sec-intro}

```{r}
#| label: setup-common-01
#| include: false
source("includes/common.R")
```

```{r}
#| label: setup-01
#| include: false
if (!identical(Sys.getenv("emlwr.eval_fits"), "true")) {
  eval_fits <- FALSE
} else {
  eval_fits <- TRUE
}

if (!eval_fits) {
  data_files <- list.files("data/01-intro", full.names = TRUE)
  data_files <- data_files[grepl("\\.Rda", data_files)]
  loaded <- lapply(data_files, load)
}
```

## Early value

To demonstrate the impact that a couple small pieces of intuition can have on execution time when evaluating machine learning models, I'll run through a quick model tuning example. On the first go, I'll lean on tidymodels' default values and a simple grid search, and on the second, I'll pull a few tricks out from my sleeves that will drastically reduce the time to evaluate models while only negligibly decreasing predictive performance.

### Setup

First, loading a few needed packages:

```{r}
#| label: load-pkgs
library(tidymodels)
library(future)
library(finetune)
library(bonsai)
```

For the purposes of this example, we'll simulate a data set of 100,000 rows and 18 columns. The first column, `class`, is a binary outcome, and the remaining variables are a mix of numerics and factors.

```{r}
#| label: d-print
set.seed(1)
d <- simulate_classification(1e5)
```

We'll first split the data into training and testing sets before generating a set of 10 folds from the training data for cross-validation.

```{r}
#| label: d-split
set.seed(1)
d_split <- initial_split(d)
d_train <- training(d_split)
d_test <- testing(d_split)
d_folds <- vfold_cv(d_train)
```

### A first go {#sec-first-go}

For my first go at tuning, I'll tune a boosted tree model using grid search. By default, tidymodels will use XGBoost as the modeling engine; I'll try out a few different values for `learn_rate`— a parameter that controls how drastically newly added trees impact predictions—and `trees`—the number of trees in the ensemble.

```{r}
#| label: bt
bt <- 
  boost_tree(learn_rate = tune(), trees = tune()) %>%
  set_mode("classification")
```

I'll carry out a grid search using `tune_grid()`, trying out a bunch of different pairs of values for `learn_rate` and `trees` and seeing what sticks. The argument `grid = 12` indicates that I want to try out 12 different combinations of values and will let tidymodels take care of exactly what those values are.

```{r}
#| label: bm-basic
#| eval: !expr eval_fits
set.seed(1)

bm_basic <- 
  bench::mark(
    basic = 
      tune_grid(
        object = bt,
        preprocessor = class ~ .,
        resamples = d_folds,
        grid = 12
      )
  )
```

```{r}
#| label: get-bm-basic
#| include: false
if (identical(eval_fits, "true")) {
  bm_basic <- trim_bench_mark(bm_basic)
  save(bm_basic, file = "data/01-intro/bm_basic.Rda")
} else {
  load("data/01-intro/bm_basic.Rda")
}
```

`bench::mark()` returns, among other things, a precise timing of how long this process takes.

```{r}
#| label: bm-basic-print
bm_basic
```

<!--# TODO: change this to 12 points so that the number of points match up -->

Holy smokes! `r round(as.numeric(bm_basic$median[[1]]) / 60 / 60, 2)` hours is a good while. What all did `tune_grid()` do, though?

First, let's break down how many model fits actually happened. Since I've supplied `grid = 12`, we're evaluating 12 possible model configurations. Each of those model configurations is evaluated against `d_folds`, a `r nrow(d_folds)`-fold cross validation object, meaning that each configuration is fitted `r nrow(d_folds)` times. That's `r nrow(d_folds) * 12` model fits!

Further, consider that those fits happen on `r nrow(d_folds)-1`/`r nrow(d_folds)`ths of the training data, or `r nrow(d_folds$splits[[1]])` rows.

<!--# TODO: The above doesn't seem to be in-lining correctly? -->

With a couple small changes, though, the time to tune this model can be *drastically* decreased.

### A speedy go {#sec-speedy-go}

To cut down on the time to evaluate these models, I'll make a few small modifications.

First, I'll **evaluate in parallel**: Almost all modern laptops have more than one CPU core, and distributing computations across them only takes a couple lines of code with tidymodels.

```{r}
plan(multisession, workers = 4)
```

While this tuning process could benefit from distributing across many more cores than 4, I'll just use 4 here to give a realistic picture of the kinds of speedups possible on a typical laptop.

Then, we'll **use a clever grid**: The tidymodels framework enables something called the "submodel trick," a technique that will allow us to predict from many more models than we actually fit. Instead of just supplying `grid = 12`, I'll construct the grid myself.

```{r}
set.seed(1)
bt_grid <-
  bt %>%
  extract_parameter_set_dials() %>% 
  grid_regular(levels = 4)
```

::: callout-note
To learn more about the submodel trick, see @sec-submodel.
:::

Next, I'll **switch out the computational engine**: Substituting XGBoost with another gradient-boosting model that can better handle some properties of this dataset will cut down on our fit time by a good bit.

```{r}
bt_lgb <- bt %>% set_engine("lightgbm")
```

Finally, I'll **give up early on poorly-performing models**: Rather than using grid search with `tune_grid()`, I'll use a technique called *racing* that stops evaluating models when they seem to be performing poorly using the `tune_race_anova()` function.

```{r}
#| label: bm-speedy
#| eval: !expr eval_fits
set.seed(1)

bm_speedy <- 
  bench::mark(
    speedy = 
      tune_race_anova(
        object = bt_lgb,
        preprocessor = class ~ .,
        resamples = d_folds,
        grid = bt_grid
      )
  )
```

```{r}
#| label: get-bm-speedy
#| include: false
if (identical(eval_fits, "true")) {
  bm_speedy <- trim_bench_mark(bm_speedy)
  save(bm_speedy, file = "data/01-intro/bm_speedy.Rda")
} else {
  load("data/01-intro/bm_speedy.Rda")
}
```

Checking out the new benchmarks:

```{r}
#| label: bm-speedy-print
bm_speedy
```

```{r}
#| label: back-to-default
#| echo: false
plan("default")
```

The total time to tune was reduced from `r round(as.numeric(bm_basic$median[[1]]) / 60 / 60, 2)` *hours* to `r round(as.numeric(bm_speedy$median[[1]]) / 60, 2)` *minutes*—the second approach was `r round(as.numeric(bm_basic[["median"]][[1]]) / as.numeric(bm_speedy[["median"]][[1]]))` times faster than the first.

The first thing I'd wonder when seeing this result is how much of a penalty in predictive performance I'd suffer due to this transition. Let's evaluate both of the top models from these tuning results on the test set. First, for the basic workflow:

```{r}
#| label: fit-basic
#| eval: !expr eval_fits
fit_basic <- 
  select_best(bm_basic$result[[1]], metric = "roc_auc") %>%
  finalize_workflow(workflow(class ~ ., bt), parameters = .) %>%
  last_fit(split = d_split)
```

```{r}
#| label: get-fit-basic
#| include: false
if (identical(eval_fits, "true")) {
  fit_basic <- trim_bench_mark(fit_basic)
  save(fit_basic, file = "data/01-intro/fit_basic.Rda")
} else {
  load("data/01-intro/fit_basic.Rda")
}
```

```{r}
#| label: print-fit-basic-metrics
collect_metrics(fit_basic)
```

As for the quicker approach:

```{r}
#| label: fit-speedy
#| eval: !expr eval_fits
fit_speedy <- 
  select_best(bm_speedy$result[[1]], metric = "roc_auc") %>%
  finalize_workflow(workflow(class ~ ., bt), parameters = .) %>%
  last_fit(split = d_split)
```

```{r}
#| label: get-fit-speedy
#| include: false
if (identical(eval_fits, "true")) {
  save(fit_speedy, file = "data/01-intro/fit_speedy.Rda")
} else {
  load("data/01-intro/fit_speedy.Rda")
}
```

```{r}
#| label: print-fit-speedy-metrics
collect_metrics(fit_speedy)
```

Virtually indistinguishable performance results in `r round(100 * as.numeric(bm_speedy[["median"]][[1]]) / as.numeric(bm_basic[["median"]][[1]]), 1)`% of the time.

## The cost of slowness

All of this said, R is not known for its computational efficiency. If I really prioritize that, why am I writing a book about R?

## Our approach

Because I use R. Evidently, you do too.

-   who is this for -- low-compute R user: why R? why tidymodels?

-   relentlessly empirical

-   what do we optimize: "overhead" of tidymodels

## The hard part

To better understand how to cut down on the time to evaluate models with tidymodels, we need to understand a bit about how tidymodels works.

Like many other "unifying frameworks" (mlr3, caret, scikit-learn(?)), the tidymodels framework itself does not implement the algorithms to train and predict from models. Instead, tidymodels provides a common interface to modeling *engines*: packages (or functions from packages) that provide the methods to `fit()` and `predict()`.

![](figures/translate_diagram.png){#fig-fit-boost-tree}

The process of "translating" between the tidymodels and engine formats is illustrated in @fig-fit-boost-tree. When fitting and predicting with tidymodels, some portion of the code's evaluation time is due to the "translation" of the inputted unified code to the specific syntax that the engine expects, and some portion of it is due to the translation of what the engine returns to the unified output returned by tidymodels; these portions are in the tidymodels team's control. The rest of the evaluation time occurs inside of the modeling engine's code.

The portions of the evaluation time that are in the tidymodels team's control are shown in green, and I'll refer to them in this book as "overhead." The overhead of tidymodels in terms of evaluation time is relatively constant with respect to the size of training data. This overhead consists of tasks like checking data types, handling errors and warnings, and—most importantly—programmatically assembling calls to engine functions.

The portion of the evaluation time shown in orange represents the actual training of (or predicting from) the model. This portion is implemented by the modeling *engine* and is thus not in the tidymodels team's control. In contrast to overhead, the evaluation time of this code is very much sensitive to the size of the inputted data; depending on the engine, increases in the number of rows or columns of training or testing data may drastically increase the time to train or predict from a given model.

::: callout-note
The algorithmic complexity of the models implemented by these engines is well-understood in many cases. At the same time, the behavior of evaluation time for some engine implementations often differs greatly from what theory would lead one to believe. Regressions in modeling code may introduce undue slowdowns and, conversely, performance optimizations that lead to evaluation times that scale better than theory would suggest may be the very reason for the existence of some engines.
:::

As shown in @fig-fit-scale, the proportion of evaluation time that overhead is responsible for depends on how quickly the engine can fit or predict for a given dataset.

![](figures/scaling_diagram.png){#fig-fit-scale}

Since the absolute overhead of tidymodels' translation is relatively constant, overhead is only a substantial portion of evaluation time when models fit or predict *very* quickly. For a linear model fitted on 30 data points with `lm()`, this overhead is [continuously benchmarked](https://github.com/tidymodels/parsnip/blob/11a3ab942f131e9d612d4d37c4b77273be064aaf/tests/testthat/test_fit_interfaces.R#L171) to remain under 2/3. That is, absolute worst-case, fitting a model with tidymodels takes three times longer than using the engine interface itself. However, this overhead approaches fractions of a percent for fits on even 10,000 rows for many engines. Thus, a focus on reducing the evaluation time of overhead is valuable in the sense that the framework ought not to unintentionally introduce regressions that cause overhead to scale with the size of training data, but in general, the hard part of reducing evaluation time when evaluating models is reducing the evaluation time for computations carried out by the modeling engine.

The next question is then *how could tidymodels cut down on evaluation time for modeling engines that it doesn't own?* To answer this question, let's revisit the applied example from @sec-first-go. In that first example, the code does some translation to the engine's syntax, sets up some error handling, and then fits and predicts from `r nrow(d_folds) * 12` models.

![](figures/basic_resample.png){#fig-basic-resample}

@fig-basic-resample depicts this process, where we evaluate all \``r nrow(d_folds) * 12` models in order. Each white dot in the engine portion of the evaluation time represents another round of fitting and predicting with engine. Remember that in reality, for even modest dataset sizes, the green portions representing tidymodels overhead are much smaller by proportion than represented.

In @sec-speedy-go, the first thing I did was introduce a parallel backend. Distributing engine fits across available cores is itself a gamechanger, as illustrated in @fig-parallel-resample.

![](figures/parallel_resample.png){#fig-parallel-resample}

Then, switching out computational engines for a more performant alternative further reduces evaluation time, as shown in @fig-parallel-resample-opt.

![](figures/parallel_resample_opt.png)

Finally, as depicted in @fig-parallel-resample-opt2, the submodel trick described in @sec-submodel and racing described in @sec-search eliminate a substantial portion of the engine fits.

![](figures/parallel_resample_opt2.png)The tidymodels team devotes substantial energy to ensuring support for the most performant parallelization technologies, modeling engines, model-specific optimizations, and search techniques. This book will demonstrate how to best make use of these features to reduce the time needed to evaluate machine learning models.

\<!--# TODO: "evaluation time" is probably ambiguous with `eval_time` *and* "evaluate models" here. find a better term for how long a thing takes to run --\>

## Datasets {#sec-datasets}

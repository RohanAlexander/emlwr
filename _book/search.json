[
  {
    "objectID": "01-intro.html#early-value",
    "href": "01-intro.html#early-value",
    "title": "1  Introduction",
    "section": "1.1 Early value",
    "text": "1.1 Early value\nTo demonstrate the impact that a couple small pieces of intuition can have on execution time when evaluating machine learning models, I’ll run through a quick model tuning example. On the first go, I’ll lean on tidymodels’ default values and a simple grid search, and on the second, I’ll pull a few tricks out from my sleeves that will drastically reduce the time to evaluate models while only negligibly decreasing predictive performance.\n\n1.1.1 Setup\nFirst, loading a few needed packages:\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5          ✔ recipes      1.0.10    \n✔ dials        1.2.1          ✔ rsample      1.2.0     \n✔ dplyr        1.1.4          ✔ tibble       3.2.1     \n✔ ggplot2      3.5.0          ✔ tidyr        1.3.1     \n✔ infer        1.0.6.9000     ✔ tune         1.2.0.9000\n✔ modeldata    1.3.0          ✔ workflows    1.1.4     \n✔ parsnip      1.2.0          ✔ workflowsets 1.0.1     \n✔ purrr        1.0.2          ✔ yardstick    1.3.0     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(future)\nlibrary(readmission)\nlibrary(finetune)\nlibrary(bonsai)\n\nFor the purposes of this example, we’ll use a dataset giving hospital readmission data for patients with Type I diabetes. The dataset includes clinical care data from 130 U.S. hospitals from years 1999-2008. Each row describes a patient “encounter” (loosely, an inpatient hospital stay).\n\nreadmission\n\n# A tibble: 71,515 × 12\n   readmitted race   sex   age   admission_source blood_glucose insurer duration\n   &lt;fct&gt;      &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;            &lt;fct&gt;         &lt;fct&gt;      &lt;dbl&gt;\n 1 Yes        Afric… Male  [60-… Referral         &lt;NA&gt;          &lt;NA&gt;           7\n 2 No         Cauca… Fema… [50-… Emergency        Normal        Private        4\n 3 Yes        Cauca… Fema… [70-… Referral         &lt;NA&gt;          Medica…        5\n 4 No         Cauca… Fema… [80-… Referral         &lt;NA&gt;          Private        5\n 5 No         Cauca… Fema… [70-… Referral         &lt;NA&gt;          &lt;NA&gt;           4\n 6 No         Cauca… Male  [50-… Emergency        Very High     &lt;NA&gt;           2\n 7 Yes        Afric… Fema… [70-… Referral         &lt;NA&gt;          Private        3\n 8 No         Cauca… Fema… [20-… Emergency        &lt;NA&gt;          &lt;NA&gt;           1\n 9 No         Cauca… Male  [60-… Other            &lt;NA&gt;          &lt;NA&gt;          12\n10 No         Cauca… Fema… [80-… Referral         &lt;NA&gt;          Medica…        1\n# ℹ 71,505 more rows\n# ℹ 4 more variables: n_previous_visits &lt;dbl&gt;, n_diagnoses &lt;dbl&gt;,\n#   n_procedures &lt;dbl&gt;, n_medications &lt;dbl&gt;\n\n\nThe first variable in this data, readmitted, gives whether the patient was readmitted within 30 days of discharge. We’d like to predict readmission using the remaining information in the dataset: demographic characteristics, insurance provider, medications taken, etc.\n\n\n\n\n\n\nNote\n\n\n\nFor a more in-depth analysis of this data, see Strack et al. (2014) and tidymodels.org (2024), as well as Section 1.5.\n\n\nWe’ll first split the data into training and testing sets before generating a set of 10 folds from the training data for cross-validation.\n\nset.seed(1)\nreadmission_split &lt;- initial_split(readmission)\nreadmission_train &lt;- training(readmission_split)\nreadmission_test &lt;- testing(readmission_split)\nreadmission_folds &lt;- vfold_cv(readmission_train)\n\n\n\n1.1.2 A first go\nFor my first go at tuning, I’ll tune a boosted tree model using grid search. By default, tidymodels will use XGBoost as the modeling engine; I’ll try out a few different values for learn_rate— a parameter that controls how drastically newly added trees impact predictions—and trees—the number of trees in the ensemble.\n\nbt &lt;- \n  boost_tree(\n    mode = \"classification\", \n    learn_rate = tune(),\n    trees = tune()\n  )\n\nI’ll carry out a grid search using tune_grid(), trying out a bunch of different pairs of values for learn_rate and trees and seeing what sticks. The argument grid = 12 indicates that I want to try out 12 different combinations of values and will let tidymodels take care of exactly what those values are.\n\nset.seed(1)\n\nbm_basic &lt;- \n  bench::mark(\n    basic = \n      tune_grid(\n        object = bt,\n        preprocessor = readmitted ~ .,\n        resamples = readmission_folds,\n        grid = 12\n      )\n  )\n\nbench::mark() returns, among other things, a precise timing of how long this process takes.\n\nbm_basic\n\n# A tibble: 1 × 13\n  expression min        median     `itr/sec` mem_alloc  `gc/sec` n_itr  n_gc\n  &lt;bnch_xpr&gt; &lt;bench_tm&gt; &lt;bench_tm&gt;     &lt;dbl&gt; &lt;bnch_byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 &lt;language&gt; 5533.004   5533.004    0.000181 5388873320  0.00795     1    44\n# ℹ 5 more variables: total_time &lt;bench_tm&gt;, result &lt;list&gt;, memory &lt;list&gt;,\n#   time &lt;list&gt;, gc &lt;list&gt;\n\n\nHoly smokes! 1.54 hours is a good while. What all did tune_grid() do, though?\nFirst, let’s break down how many model fits actually happened. Since I’ve supplied grid = 12, we’re evaluating 12 possible model configurations. Each of those model configurations is evaluated against readmission_folds, a 10-fold cross validation object, meaning that each configuration is fitted 10 times. That’s 120 model fits!\nFurther, consider that those fits happen on 9/10ths of the training data, or 48272 rows.\n\nWith a couple small changes, though, the time to tune this model can be drastically decreased.\n\n\n1.1.3 A speedy go\nTo cut down on the time to evaluate these models, I’ll make a few small modifications.\nFirst, I’ll evaluate in parallel: Almost all modern laptops have more than one CPU core, and distributing computations across them only takes a couple lines of code with tidymodels.\n\nplan(multisession, workers = 4)\n\nWhile this tuning process could benefit from distributing across many more cores than 4, I’ll just use 4 here to give a realistic picture of the kinds of speedups possible on a typical laptop.\nThen, we’ll use a clever grid: The tidymodels framework enables something called the “submodel trick,” a technique that will allow us to predict from many more models than we actually fit. Instead of just supplying grid = 12, I’ll construct the grid myself.\n\nbt_grid &lt;-\n  bt %&gt;%\n  extract_parameter_set_dials() %&gt;% \n  grid_regular(levels = 4)\n\n\n\n\n\n\n\nNote\n\n\n\nTo learn more about the submodel trick, see Section 2.1.\n\n\nNext, I’ll switch out the computational engine: Substituting XGBoost with another gradient-boosting model that can better handle some properties of this dataset will cut down on our fit time by a good bit.\n\nbt_lgb &lt;- bt %&gt;% set_engine(\"lightgbm\")\n\nFinally, I’ll give up early on poorly-performing models: Rather than using grid search with tune_grid(), I’ll use a technique called racing that stops evaluating models when they seem to be performing poorly using the tune_race_anova() function.\n\nset.seed(1)\n\nbm_speedy &lt;- \n  bench::mark(\n    speedy = \n      tune_race_anova(\n        object = bt_lgb,\n        preprocessor = readmitted ~ .,\n        resamples = readmission_folds,\n        grid = bt_grid\n      )\n  )\n\nChecking out the new benchmarks:\n\nbm_speedy\n\n# A tibble: 1 × 13\n  expression min        median     `itr/sec` mem_alloc  `gc/sec` n_itr  n_gc\n  &lt;bnch_xpr&gt; &lt;bench_tm&gt; &lt;bench_tm&gt;     &lt;dbl&gt; &lt;bnch_byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 &lt;language&gt; 197.9526   197.9526     0.00505 1774148152    0.399     1    79\n# ℹ 5 more variables: total_time &lt;bench_tm&gt;, result &lt;list&gt;, memory &lt;list&gt;,\n#   time &lt;list&gt;, gc &lt;list&gt;\n\n\nThe total time to tune was reduced from 1.54 hours to 3.3 minutes—the second approach was 28 times faster than the first.\nThe first thing I’d wonder when seeing this result is how much of a penalty in predictive performance I’d suffer due to this transition. Let’s evaluate both of the top models from these tuning results on the test set. First, for the basic workflow:\n\nfit_basic &lt;- \n  select_best(bm_basic$result[[1]], metric = \"roc_auc\") %&gt;%\n  finalize_workflow(workflow(readmitted ~ ., bt), parameters = .) %&gt;%\n  last_fit(split = readmission_split)\n\n\ncollect_metrics(fit_basic)\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary        0.912  Preprocessor1_Model1\n2 roc_auc     binary        0.596  Preprocessor1_Model1\n3 brier_class binary        0.0799 Preprocessor1_Model1\n\n\nAs for the quicker approach:\n\nfit_speedy &lt;- \n  select_best(bm_speedy$result[[1]], metric = \"roc_auc\") %&gt;%\n  finalize_workflow(workflow(readmitted ~ ., bt), parameters = .) %&gt;%\n  last_fit(split = readmission_split)\n\n\ncollect_metrics(fit_speedy)\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary        0.912  Preprocessor1_Model1\n2 roc_auc     binary        0.593  Preprocessor1_Model1\n3 brier_class binary        0.0829 Preprocessor1_Model1\n\n\nVirtually indistinguishable performance results in 3.6% of the time."
  },
  {
    "objectID": "01-intro.html#the-cost-of-slowness",
    "href": "01-intro.html#the-cost-of-slowness",
    "title": "1  Introduction",
    "section": "1.2 The cost of slowness",
    "text": "1.2 The cost of slowness\nAll of this said, R is not known for its computational efficiency. If I really prioritize that, why am I writing a book about R?"
  },
  {
    "objectID": "01-intro.html#our-approach",
    "href": "01-intro.html#our-approach",
    "title": "1  Introduction",
    "section": "1.3 Our approach",
    "text": "1.3 Our approach\nBecause I use R. Evidently, you do too.\n\nwho is this for – low-compute R user: why R? why tidymodels?\nrelentlessly empirical\nwhat do we optimize: “overhead” of tidymodels"
  },
  {
    "objectID": "01-intro.html#sec-datasets",
    "href": "01-intro.html#sec-datasets",
    "title": "1  Introduction",
    "section": "1.5 Datasets",
    "text": "1.5 Datasets\n\n\n\n\nStrack, Beata, Jonathan P DeShazo, Chris Gennings, Juan L Olmo, Sebastian Ventura, Krzysztof J Cios, John N Clore, et al. 2014. “Impact of HbA1c Measurement on Hospital Readmission Rates: Analysis of 70,000 Clinical Database Patient Records.” BioMed Research International 2014.\n\n\ntidymodels.org. 2024. “Fair Prediction of Hospital Readmission: A Machine Learning Fairness Case Study.” https://www.tidymodels.org/learn/work/fairness-readmission/."
  },
  {
    "objectID": "02-models.html#sec-submodel",
    "href": "02-models.html#sec-submodel",
    "title": "2  Models",
    "section": "2.1 The submodel trick",
    "text": "2.1 The submodel trick"
  },
  {
    "objectID": "03-preprocessing.html",
    "href": "03-preprocessing.html",
    "title": "3  Preprocessors",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "04-tuning.html",
    "href": "04-tuning.html",
    "title": "4  Search",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "tidymodels.org. 2024. “Fair Prediction of Hospital Readmission: A\nMachine Learning Fairness Case Study.” https://www.tidymodels.org/learn/work/fairness-readmission/."
  },
  {
    "objectID": "01-intro.html#the-hard-part",
    "href": "01-intro.html#the-hard-part",
    "title": "1  Introduction",
    "section": "1.4 The hard part",
    "text": "1.4 The hard part\nTo better understand how to cut down on the time to evaluate models with tidymodels, we need to understand a bit about how tidymodels works.\nLike many other “unifying frameworks” (mlr3, caret, scikit-learn(?)), the tidymodels framework itself does not implement the algorithms to train and predict from models. Instead, tidymodels provides a common interface to modeling engines: packages (or functions from packages) that provide the methods to fit() and predict().\n\nThe process of “translating” between the tidymodels and engine formats is illustrated in ?fig-fit-boost-tree. When fitting and predicting with tidymodels, some portion of the code’s evaluation time is due to the “translation” of the inputted unified code to the specific syntax that the engine expects, and some portion of it is due to the translation of what the engine returns to the unified output returned by tidymodels; these portions are in the tidymodels team’s control. The rest of the evaluation time occurs inside of the modeling engine’s code.\nThe portions of the evaluation time that are in the tidymodels team’s control are shown in green, and I’ll refer to them in this book as “overhead.” The overhead of tidymodels in terms of evaluation time is relatively constant with respect to the size of training data. This overhead consists of tasks like checking data types, handling errors and warnings, and—most importantly—programmatically assembling calls to engine functions.\nThe portion of the evaluation time shown in orange represents the actual training of (or predicting from) the model. This portion is implemented by the modeling engine and is thus not in the tidymodels team’s control. In contrast to overhead, the evaluation time of this code is very much sensitive to the size of the inputted data; depending on the engine, increases in the number of rows or columns of training or testing data may drastically increase the time to train or predict from a given model.\n\n\n\n\n\n\nNote\n\n\n\nThe algorithmic complexity of the models implemented by these engines is well-understood in many cases. At the same time, the behavior of evaluation time for some engine implementations often differs greatly from what theory would lead one to believe. Regressions in modeling code may introduce undue slowdowns and, conversely, performance optimizations that lead to evaluation times that scale better than theory would suggest may be the very reason for the existence of some engines.\n\n\nAs shown in ?fig-fit-scale, the proportion of evaluation time that overhead is responsible for depends on how quickly the engine can fit or predict for a given dataset.\n\nSince the absolute overhead of tidymodels’ translation is relatively constant, overhead is only a substantial portion of evaluation time when models fit or predict very quickly. For a linear model fitted on 30 data points with lm(), this overhead is continuously benchmarked to remain under 2/3. That is, absolute worst-case, fitting a model with tidymodels takes three times longer than using the engine interface itself. However, this overhead approaches fractions of a percent for fits on even 10,000 rows for many engines. Thus, a focus on reducing the evaluation time of overhead is valuable in the sense that the framework ought not to unintentionally introduce regressions that cause overhead to scale with the size of training data, but in general, the hard part of reducing evaluation time when evaluating models is reducing the evaluation time for computations carried out by the modeling engine.\nThe next question is then how could tidymodels cut down on evaluation time for modeling engines that it doesn’t own? To answer this question, let’s revisit the applied example from Section 1.1.2. In that first example, the code does some translation to the engine’s syntax, sets up some error handling, and then fits and predicts from 120 models.\n\n?fig-basic-resample depicts this process, where we evaluate all `120 models in order. Each white dot in the engine portion of the evaluation time represents another round of fitting and predicting with engine. Remember that in reality, for even modest dataset sizes, the green portions representing tidymodels overhead are much smaller by proportion than represented.\nIn Section 1.1.3, the first thing I did was introduce a parallel backend. Distributing engine fits across available cores is itself a gamechanger, as illustrated in ?fig-parallel-resample.\n\nThen, switching out computational engines for a more performant alternative further reduces evaluation time, as shown in ?fig-parallel-resample-opt.\n\nFinally, as depicted in ?fig-parallel-resample-opt2, the submodel trick described in Section 2.1 and racing described in Chapter 4 eliminate a substantial portion of the engine fits.\nThe tidymodels team devotes substantial energy to ensuring support for the most performant parallelization technologies, modeling engines, model-specific optimizations, and search techniques. This book will demonstrate how to best make use of these features to reduce the time needed to evaluate machine learning models.\n&lt;!–# TODO: “evaluation time” is probably ambiguous with eval_time and “evaluate models” here. find a better term for how long a thing takes to run –&gt;"
  }
]